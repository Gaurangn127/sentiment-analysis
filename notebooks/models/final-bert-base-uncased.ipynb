{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":115439,"databundleVersionId":13800781,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":64.923296,"end_time":"2025-11-21T17:12:53.341877","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-11-21T17:11:48.418581","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"0170708e81004a61b38e482d88eb6fc6":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"057cfa12609d4532a38219475503762e":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_35fc379af795489592f330c69a29951b","max":18,"min":0,"orientation":"horizontal","style":"IPY_MODEL_19a3eabc43524caba3460bcf05f2b8ea","tabbable":null,"tooltip":null,"value":18}},"0cb5983a604842d482df13c89bfd1cfb":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0cfdba0ecc74443998f6fe6de631ff49":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_fff677f8fb474db2b4ae064b25db95e2","placeholder":"​","style":"IPY_MODEL_a2f0b8a723c84d449fa1de2c25e06e73","tabbable":null,"tooltip":null,"value":" 2.46M/2.46M [00:00&lt;00:00, 5.92MB/s]"}},"1019a320116b4c6a9f8d4d24f7e15b4d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"14fc7f88e272405ca8e9c88cef2c838d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6f55d3771efb47de89d30c85c5c511fa","IPY_MODEL_40432373f2b541fab0ff53091c33a6c5","IPY_MODEL_0cfdba0ecc74443998f6fe6de631ff49"],"layout":"IPY_MODEL_3c79110480c642b8b32b7647a5aca78b","tabbable":null,"tooltip":null}},"19a3eabc43524caba3460bcf05f2b8ea":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2abd9f97ccce46b7a831ebe5ddd4aa62":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_4ec5c97f7ea345b884005d7bb6bcae91","placeholder":"​","style":"IPY_MODEL_b5c9693a75b6431fa52d5a12302bb8d4","tabbable":null,"tooltip":null,"value":"added_tokens.json: 100%"}},"2ba416ae280c4e96a9959d8bb3b150b4":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35fc379af795489592f330c69a29951b":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38efe3ba1f044269b0f59b759e9f79c0":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2abd9f97ccce46b7a831ebe5ddd4aa62","IPY_MODEL_057cfa12609d4532a38219475503762e","IPY_MODEL_4983e3b2f1bd48d6ae4c3ed41de448b5"],"layout":"IPY_MODEL_c7a4b6e5ca51456699756d1dabf64641","tabbable":null,"tooltip":null}},"3b79f24a4e9847fdb5ded56e22cd1abc":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"3c79110480c642b8b32b7647a5aca78b":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40432373f2b541fab0ff53091c33a6c5":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_f08a85095b0b448ab8e09de0ecf2088c","max":2464616,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c70fee349c15493bb47989e4b7651dfb","tabbable":null,"tooltip":null,"value":2464616}},"4983e3b2f1bd48d6ae4c3ed41de448b5":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_0cb5983a604842d482df13c89bfd1cfb","placeholder":"​","style":"IPY_MODEL_c3509f28c1e3423591ab17065010f195","tabbable":null,"tooltip":null,"value":" 18.0/18.0 [00:00&lt;00:00, 2.39kB/s]"}},"4ec5c97f7ea345b884005d7bb6bcae91":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62f2178edefb4bdfb7c391299d45f37d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"66109d7ebcfd4159ae90b8b7131ebd9a":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_ed740b0470aa43f8886a436ea3ebc39c","placeholder":"​","style":"IPY_MODEL_62f2178edefb4bdfb7c391299d45f37d","tabbable":null,"tooltip":null,"value":"special_tokens_map.json: 100%"}},"6b57d03ef18245e69b3c4afac4a92294":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ebd2924e1754a56b7e4138cc8a99d4e":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f55d3771efb47de89d30c85c5c511fa":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_f4fe40a963cd4545aefeb69046607725","placeholder":"​","style":"IPY_MODEL_c31c2a1cc0094b95978a993172991721","tabbable":null,"tooltip":null,"value":"spm.model: 100%"}},"76300340c69946b881e0f194ed9c2246":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c973269c0ed4fd3be374a1b08f9ff85":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_abedac7d07844c3f9da7bed46bc7b618","placeholder":"​","style":"IPY_MODEL_d5339a0407ab40ce9b21c77a7f0a9508","tabbable":null,"tooltip":null,"value":" 156/156 [00:00&lt;00:00, 21.3kB/s]"}},"96484a52a40d4b789fac427abd681ed0":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"a2f0b8a723c84d449fa1de2c25e06e73":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"abedac7d07844c3f9da7bed46bc7b618":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b06094e3a46d40fbb3addca53dd15326":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_76300340c69946b881e0f194ed9c2246","placeholder":"​","style":"IPY_MODEL_96484a52a40d4b789fac427abd681ed0","tabbable":null,"tooltip":null,"value":"tokenizer_config.json: 100%"}},"b5c9693a75b6431fa52d5a12302bb8d4":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"c31c2a1cc0094b95978a993172991721":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"c3509f28c1e3423591ab17065010f195":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"c70fee349c15493bb47989e4b7651dfb":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c7a4b6e5ca51456699756d1dabf64641":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cda087a5be544b1fa8c23d2f35d5d19e":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b06094e3a46d40fbb3addca53dd15326","IPY_MODEL_f116738cf5d94d59b55135f57198006e","IPY_MODEL_e433c1cbb44c46e9b42f4b27eb189b0e"],"layout":"IPY_MODEL_d84929aad8d843afbbfa92fa508622e5","tabbable":null,"tooltip":null}},"cef6a4c9dee14a85bf95cd8daef5dc60":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cf9584d3556f4a208e7dfed993989057":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_66109d7ebcfd4159ae90b8b7131ebd9a","IPY_MODEL_eee75d6d171648a19a7d3548e02641d8","IPY_MODEL_8c973269c0ed4fd3be374a1b08f9ff85"],"layout":"IPY_MODEL_2ba416ae280c4e96a9959d8bb3b150b4","tabbable":null,"tooltip":null}},"d5339a0407ab40ce9b21c77a7f0a9508":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"d84929aad8d843afbbfa92fa508622e5":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e433c1cbb44c46e9b42f4b27eb189b0e":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_6ebd2924e1754a56b7e4138cc8a99d4e","placeholder":"​","style":"IPY_MODEL_3b79f24a4e9847fdb5ded56e22cd1abc","tabbable":null,"tooltip":null,"value":" 372/372 [00:00&lt;00:00, 37.3kB/s]"}},"ed740b0470aa43f8886a436ea3ebc39c":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eee75d6d171648a19a7d3548e02641d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_0170708e81004a61b38e482d88eb6fc6","max":156,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1019a320116b4c6a9f8d4d24f7e15b4d","tabbable":null,"tooltip":null,"value":156}},"f08a85095b0b448ab8e09de0ecf2088c":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f116738cf5d94d59b55135f57198006e":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_6b57d03ef18245e69b3c4afac4a92294","max":372,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cef6a4c9dee14a85bf95cd8daef5dc60","tabbable":null,"tooltip":null,"value":372}},"f4fe40a963cd4545aefeb69046607725":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fff677f8fb474db2b4ae064b25db95e2":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"74f8881e","cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport wandb\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport kagglehub\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport string # for milestone 1\nfrom collections import Counter\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, precision_recall_curve, average_precision_score, accuracy_score\n\nfrom datasets import Dataset, DatasetDict\nfrom torch.utils.data import DataLoader, Dataset as TorchDataset\nfrom transformers import (\n        AutoTokenizer, \n        AutoModelForSequenceClassification,\n        AutoConfig\n    )\n\nfrom torch.optim import AdamW # Import AdamW from torch\nfrom torch.optim.lr_scheduler import StepLR # Import StepLR\n\nfrom kaggle_secrets import UserSecretsClient # For secure API key access\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" # getting rid of hugging face tokenizer parallelism warnings\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\n# Set multiprocessing start method \n# This can prevent deadlocks in notebook environments\ntry:\n    torch.multiprocessing.set_start_method(\"spawn\", force=True)\n    print(\"\\nSet torch multiprocessing start method to 'spawn'.\")\nexcept RuntimeError as e:\n    print(f\"Note: Could not set start method: {e}\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-11-26T15:32:28.521281Z","iopub.execute_input":"2025-11-26T15:32:28.521544Z","iopub.status.idle":"2025-11-26T15:32:41.630986Z","shell.execute_reply.started":"2025-11-26T15:32:28.521513Z","shell.execute_reply":"2025-11-26T15:32:41.630142Z"},"papermill":{"duration":12.382448,"end_time":"2025-11-21T17:12:04.944323","exception":false,"start_time":"2025-11-21T17:11:52.561875","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/2025-sep-dl-gen-ai-project/sample_submission.csv\n/kaggle/input/2025-sep-dl-gen-ai-project/train.csv\n/kaggle/input/2025-sep-dl-gen-ai-project/test.csv\n\nSet torch multiprocessing start method to 'spawn'.\n","output_type":"stream"}],"execution_count":1},{"id":"10413023","cell_type":"markdown","source":"# 1. Configuration & Globals","metadata":{"execution":{"iopub.execute_input":"2025-11-19T13:47:40.326147Z","iopub.status.busy":"2025-11-19T13:47:40.325708Z","iopub.status.idle":"2025-11-19T13:47:40.329857Z","shell.execute_reply":"2025-11-19T13:47:40.329252Z","shell.execute_reply.started":"2025-11-19T13:47:40.326126Z"},"papermill":{"duration":0.006029,"end_time":"2025-11-21T17:12:04.957320","exception":false,"start_time":"2025-11-21T17:12:04.951291","status":"completed"},"tags":[]}},{"id":"c575dbf0","cell_type":"code","source":"# Set to True for the first run- training and model upload\n# Set to False while submitting the notebook\nDO_TRAIN_AND_UPLOAD = True\n\n# Set this to True to train on 100% of data (no validation split).\nUSE_FULL_DATA = False \n\n# Thresholds found in previous optimal tuning runs\n# Used when USE_FULL_DATA=True or during inference\n# MANUAL_THRESHOLDS = [0.99, 0.54, 0.88, 0.7100000000000001, 0.52] # bert\n# MANUAL_THRESHOLDS = [0.8, 0.25, 0.77, 0.59, 0.86] # deberta, 21 epochs, using full data\nMANUAL_THRESHOLDS = [0.63, 0.17, 0.93, 0.56, 0.85]\n\n# Set to true for ensembling 2 models together during inference\nENSEMBLE_MODELS = False","metadata":{"execution":{"iopub.status.busy":"2025-11-26T16:02:56.184434Z","iopub.execute_input":"2025-11-26T16:02:56.185086Z","iopub.status.idle":"2025-11-26T16:02:56.189022Z","shell.execute_reply.started":"2025-11-26T16:02:56.185063Z","shell.execute_reply":"2025-11-26T16:02:56.188250Z"},"papermill":{"duration":0.011521,"end_time":"2025-11-21T17:12:04.974989","exception":false,"start_time":"2025-11-21T17:12:04.963468","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":16},{"id":"c5212a54","cell_type":"code","source":"config = {\n    \"MODEL_NAME\": \"bert-base-uncased\", \n    \"TRAIN_FILE\": \"/kaggle/input/2025-sep-dl-gen-ai-project/train.csv\",\n    \"TEST_FILE\": \"/kaggle/input/2025-sep-dl-gen-ai-project/test.csv\",\n    \"VALIDATION_SPLIT_SIZE\": 0.1, \n    \"EPOCHS\": 21, \n    \"STARTING_LR\": 5e-5,\n    \"TRAIN_BATCH_SIZE\": 16, \n    \"EVAL_BATCH_SIZE\": 32, \n    \"CLASSIFIER_DROPOUT\": 0.1, \n    \"RANDOM_SEED\": 42, \n    \"MAX_TOKEN_LENGTH\": 128,\n    \"LR_BASE_FACTOR\": 1, # for discriminative learning rate\n}","metadata":{"execution":{"iopub.status.busy":"2025-11-26T15:32:41.638465Z","iopub.execute_input":"2025-11-26T15:32:41.638754Z","iopub.status.idle":"2025-11-26T15:32:41.659820Z","shell.execute_reply.started":"2025-11-26T15:32:41.638729Z","shell.execute_reply":"2025-11-26T15:32:41.658976Z"},"papermill":{"duration":0.011594,"end_time":"2025-11-21T17:12:04.993034","exception":false,"start_time":"2025-11-21T17:12:04.981440","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":3},{"id":"e0c7e4e1","cell_type":"code","source":"# defining labels for the task\nemotion_labels = ['anger', 'fear', 'joy', 'sadness', 'surprise']\n\nlabel2id = {label: i for i, label in enumerate(emotion_labels)}\nid2label = {i: label for i, label in enumerate(emotion_labels)}\n\nnum_labels = len(emotion_labels)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(f\"Using device: {device}\")","metadata":{"execution":{"iopub.status.busy":"2025-11-26T15:32:41.660899Z","iopub.execute_input":"2025-11-26T15:32:41.661285Z","iopub.status.idle":"2025-11-26T15:32:41.737705Z","shell.execute_reply.started":"2025-11-26T15:32:41.661252Z","shell.execute_reply":"2025-11-26T15:32:41.736781Z"},"papermill":{"duration":0.094877,"end_time":"2025-11-21T17:12:05.094024","exception":false,"start_time":"2025-11-21T17:12:04.999147","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":4},{"id":"90c9bcfa","cell_type":"markdown","source":"# 2. Data Loading & Preparation","metadata":{"papermill":{"duration":0.006167,"end_time":"2025-11-21T17:12:05.107403","exception":false,"start_time":"2025-11-21T17:12:05.101236","status":"completed"},"tags":[]}},{"id":"ee7756af","cell_type":"code","source":"# loading data\nall_train_df = pd.read_csv(config[\"TRAIN_FILE\"])\ntest_df = pd.read_csv(config[\"TEST_FILE\"])\n\nif USE_FULL_DATA:\n    df_train = all_train_df\n    df_val = pd.DataFrame() # empty dataframe\n\nelse:\n    # create a train-test-split as in the config\n    df_train, df_val = train_test_split(\n        all_train_df,\n        test_size=config[\"VALIDATION_SPLIT_SIZE\"],\n        random_state=config[\"RANDOM_SEED\"]\n    )\n    \nprint(f\"Training split shape: {df_train.shape}\")\nprint(f\"Validation split shape: {df_val.shape}\")","metadata":{"execution":{"iopub.status.busy":"2025-11-26T15:32:41.738604Z","iopub.execute_input":"2025-11-26T15:32:41.739805Z","iopub.status.idle":"2025-11-26T15:32:41.807597Z","shell.execute_reply.started":"2025-11-26T15:32:41.739781Z","shell.execute_reply":"2025-11-26T15:32:41.806941Z"},"papermill":{"duration":0.055138,"end_time":"2025-11-21T17:12:05.168787","exception":false,"start_time":"2025-11-21T17:12:05.113649","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Training split shape: (6144, 8)\nValidation split shape: (683, 8)\n","output_type":"stream"}],"execution_count":5},{"id":"d88a4e91","cell_type":"code","source":"# calc pos_vector \n# We calculate weights based on the data we are training on\npos_weights_list = []\n\ntotal_train_samples = len(df_train)\n\nfor label in emotion_labels:\n    pos_count = df_train[label].sum()\n    neg_count = total_train_samples - pos_count\n    weight = neg_count / pos_count if pos_count > 0 else 1.0\n    pos_weights_list.append(weight)\n    \npos_weight_tensor = torch.tensor(pos_weights_list, dtype=torch.float).to(device)\n\nprint(f\"pos_weight vector: {pos_weights_list}\")","metadata":{"execution":{"iopub.status.busy":"2025-11-26T15:32:41.808376Z","iopub.execute_input":"2025-11-26T15:32:41.808585Z","iopub.status.idle":"2025-11-26T15:32:41.985899Z","shell.execute_reply.started":"2025-11-26T15:32:41.808568Z","shell.execute_reply":"2025-11-26T15:32:41.985109Z"},"papermill":{"duration":0.279258,"end_time":"2025-11-21T17:12:05.454488","exception":false,"start_time":"2025-11-21T17:12:05.175230","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"pos_weight vector: [7.43956043956044, 0.7716262975778547, 3.1042084168336674, 2.133095359510454, 2.417130144605117]\n","output_type":"stream"}],"execution_count":6},{"id":"a8622852","cell_type":"code","source":"# Loading tokenizer\ntokenizer = AutoTokenizer.from_pretrained(config[\"MODEL_NAME\"])","metadata":{"execution":{"iopub.status.busy":"2025-11-26T15:32:41.986758Z","iopub.execute_input":"2025-11-26T15:32:41.986956Z","iopub.status.idle":"2025-11-26T15:32:45.480344Z","shell.execute_reply.started":"2025-11-26T15:32:41.986940Z","shell.execute_reply":"2025-11-26T15:32:45.479714Z"},"papermill":{"duration":3.336362,"end_time":"2025-11-21T17:12:08.797345","exception":false,"start_time":"2025-11-21T17:12:05.460983","status":"completed"},"tags":[],"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"876dc5b293e94f17a13dc27ea3309756"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95e6cc73caad416eb2499557508b914a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccceb27c131c4ee6ae6e954ce0c796a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"249b92e3e00140e1bc81a6f955fddf39"}},"metadata":{}}],"execution_count":7},{"id":"94e0d438","cell_type":"code","source":"def preprocess_function(batch_texts, batch_labels):\n    \"\"\"Tokenizes text and returns a dict for the model.\"\"\"\n    tokenized_inputs = tokenizer(\n        batch_texts,\n        truncation=True,\n        padding=\"max_length\",\n        max_length=config[\"MAX_TOKEN_LENGTH\"],\n        return_tensors=\"pt\"\n    )\n    labels_tensor = torch.tensor(batch_labels, dtype=torch.float)\n    return {\n        \"input_ids\": tokenized_inputs[\"input_ids\"],\n        \"attention_mask\": tokenized_inputs[\"attention_mask\"],\n        \"labels\": labels_tensor\n    }","metadata":{"execution":{"iopub.status.busy":"2025-11-26T15:32:45.481088Z","iopub.execute_input":"2025-11-26T15:32:45.481777Z","iopub.status.idle":"2025-11-26T15:32:45.486705Z","shell.execute_reply.started":"2025-11-26T15:32:45.481748Z","shell.execute_reply":"2025-11-26T15:32:45.485906Z"},"papermill":{"duration":0.012538,"end_time":"2025-11-21T17:12:08.817165","exception":false,"start_time":"2025-11-21T17:12:08.804627","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":8},{"id":"701ccfad","cell_type":"code","source":"class EmotionDataset(TorchDataset):\n    \"\"\"Custom PyTorch dataset.\"\"\"\n    def __init__(self, df, is_test=False):\n        self.texts = df['text'].tolist()\n        self.is_test = is_test\n        if not self.is_test:\n            self.labels = df[emotion_labels].values\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        if self.is_test:\n            return self.texts[idx], [] # Return empty labels for test\n        return self.texts[idx], self.labels[idx]","metadata":{"execution":{"iopub.status.busy":"2025-11-26T15:32:45.488785Z","iopub.execute_input":"2025-11-26T15:32:45.489012Z","iopub.status.idle":"2025-11-26T15:32:45.503794Z","shell.execute_reply.started":"2025-11-26T15:32:45.488994Z","shell.execute_reply":"2025-11-26T15:32:45.502909Z"},"papermill":{"duration":0.012128,"end_time":"2025-11-21T17:12:08.835782","exception":false,"start_time":"2025-11-21T17:12:08.823654","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":9},{"id":"f3ec6dd1","cell_type":"code","source":"def collate_fn(batch):\n    \"\"\"Custom collate function to batch-tokenize.\"\"\"\n    texts, labels = zip(*batch)\n    return preprocess_function(list(texts), list(labels))\n\ndef collate_fn_test(batch):\n    \"\"\"Collate function for the test set (no labels).\"\"\"\n    texts, _ = zip(*batch)\n    tokenized_inputs = tokenizer(\n        list(texts),\n        truncation=True,\n        padding=\"max_length\",\n        max_length=config[\"MAX_TOKEN_LENGTH\"],\n        return_tensors=\"pt\"\n    )\n    return {\n        \"input_ids\": tokenized_inputs[\"input_ids\"],\n        \"attention_mask\": tokenized_inputs[\"attention_mask\"]\n    }","metadata":{"execution":{"iopub.status.busy":"2025-11-26T15:32:45.504551Z","iopub.execute_input":"2025-11-26T15:32:45.504822Z","iopub.status.idle":"2025-11-26T15:32:45.518767Z","shell.execute_reply.started":"2025-11-26T15:32:45.504792Z","shell.execute_reply":"2025-11-26T15:32:45.518108Z"},"papermill":{"duration":0.012324,"end_time":"2025-11-21T17:12:08.854820","exception":false,"start_time":"2025-11-21T17:12:08.842496","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":10},{"id":"e47a7742","cell_type":"code","source":"# Create DataLoaders\ntrain_dataset = EmotionDataset(df_train)\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=config[\"TRAIN_BATCH_SIZE\"],\n    collate_fn=collate_fn,\n    shuffle=True,\n    num_workers=0 # Prevent notebook deadlocks\n)\n\n# Only create val_loader if we are NOT using full data\nif not USE_FULL_DATA:\n    val_dataset = EmotionDataset(df_val)\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=config[\"EVAL_BATCH_SIZE\"],\n        collate_fn=collate_fn,\n        shuffle=False,\n        num_workers=0\n    )\nelse:\n    val_loader = None\n\n# Always create test loader\ntest_dataset_obj = EmotionDataset(test_df, is_test=True)\ntest_loader = DataLoader(\n    test_dataset_obj,\n    batch_size=config[\"EVAL_BATCH_SIZE\"],\n    collate_fn=collate_fn_test,\n    shuffle=False,\n    num_workers=0 # Prevent notebook deadlocks\n)\n\nsteps_per_epoch = len(train_loader)\nprint(f\"Steps per epoch: {steps_per_epoch}\")","metadata":{"execution":{"iopub.status.busy":"2025-11-26T15:32:45.519602Z","iopub.execute_input":"2025-11-26T15:32:45.519864Z","iopub.status.idle":"2025-11-26T15:32:45.543727Z","shell.execute_reply.started":"2025-11-26T15:32:45.519834Z","shell.execute_reply":"2025-11-26T15:32:45.542942Z"},"papermill":{"duration":0.017686,"end_time":"2025-11-21T17:12:08.879096","exception":false,"start_time":"2025-11-21T17:12:08.861410","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Steps per epoch: 384\n","output_type":"stream"}],"execution_count":11},{"id":"444a86ef","cell_type":"markdown","source":"# 3. Training, WandB Logging & Upload","metadata":{"papermill":{"duration":0.00645,"end_time":"2025-11-21T17:12:08.892269","exception":false,"start_time":"2025-11-21T17:12:08.885819","status":"completed"},"tags":[]}},{"id":"18425a5c","cell_type":"code","source":"if DO_TRAIN_AND_UPLOAD:\n    # 1. WandB Init \n    os.environ[\"WANDB_SILENT\"] = \"true\"\n    try:\n        user_secrets = UserSecretsClient()\n        api_key = user_secrets.get_secret(\"wandb_api\")\n        wandb.login(key=api_key)\n        print(\"W&B login successful.\")\n        run = wandb.init(\n            project=\"multi-label-emotion-bert\",\n            job_type=\"train-upload\",\n            config=config,\n            name=f\"FINAL-{config['MODEL_NAME']}-{wandb.util.generate_id()}\"\n        )\n        wandb.config.update({\"pos_weights\": pos_weights_list})\n    except Exception as e:\n        print(f\"W&B init failed: {e}\")\n\n    print(\"\\n--- Starting Model Training ---\")\n\n    # 2. Model & Optimizer Setup\n    model = AutoModelForSequenceClassification.from_pretrained(\n        config[\"MODEL_NAME\"],\n        num_labels=num_labels,  # This forces it to shape (batch_Size, 5)\n        problem_type=\"multi_label_classification\",\n        ignore_mismatched_sizes=True \n    ).to(device)\n\n    # Discriminative LR (DLR) Implementation\n    base_lr = config[\"STARTING_LR\"] * config[\"LR_BASE_FACTOR\"]\n    head_lr = config[\"STARTING_LR\"]                             \n    \n    optimizer_grouped_parameters = [\n        {\n            \"params\": model.bert.parameters(), # deBERTa encoder parameters\n            \"lr\": base_lr,\n            \"weight_decay\": 0.01\n        },\n        {\n            \"params\": model.classifier.parameters(), # Classification head parameters\n            \"lr\": head_lr,\n            \"weight_decay\": 0.0\n        }\n    ]\n    \n    optimizer = AdamW(optimizer_grouped_parameters, lr=head_lr)\n\n    print(\"Using StepLR scheduler\")\n    scheduler = StepLR(\n        optimizer,\n        step_size=2, # Decay every 2 epochs\n        gamma=0.5    # Halve the learning rate\n    )\n\n    loss_fct = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n\n    best_val_f1 = -1.0\n    best_model_state = None\n    global_step = 0\n\n    if MANUAL_THRESHOLDS:\n        thresholds_tensor = torch.tensor(MANUAL_THRESHOLDS).to(device)\n\n    # --- 3. Training Loop ---\n    for epoch in range(config[\"EPOCHS\"]):\n        print(f\"\\n--- Starting Epoch {epoch+1}/{config['EPOCHS']} ---\")\n        model.train() \n        \n        train_preds_list = []\n        train_labels_list = []\n        \n        total_loss = 0\n        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\", disable=False):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            \n            outputs = model(input_ids, attention_mask=attention_mask)\n            logits = outputs.logits\n            loss = loss_fct(logits, labels)\n            \n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            total_loss += loss.item()\n\n            probs = torch.sigmoid(logits)\n            \n            if MANUAL_THRESHOLDS :\n                preds = (probs > thresholds_tensor).int()\n            else: \n                preds = (probs > 0.5).int()\n                \n            # 3. Detach, move to CPU, convert to numpy to save GPU memory\n            train_preds_list.append(preds.detach().cpu().numpy())\n            train_labels_list.append(labels.detach().cpu().numpy())\n            \n            # WandB Log Step\n            if run is not None:\n                wandb.log({\n                    \"train/step_loss\": loss.item(),\n                    \"train/lr_head\": optimizer.param_groups[1]['lr'],\n                    \"train/lr_base\": optimizer.param_groups[0]['lr'],\n                    \"global_step\": global_step\n                })\n            global_step += 1\n            \n        avg_train_loss = total_loss / len(train_loader)\n            \n        all_train_preds = np.concatenate(train_preds_list, axis=0)\n        all_train_labels = np.concatenate(train_labels_list, axis=0)\n        train_f1 = f1_score(all_train_labels, all_train_preds, average='macro', zero_division=0)\n        train_acc = accuracy_score(all_train_labels, all_train_preds)\n        \n        print(f\"  Average Training Loss: {avg_train_loss:.4f}\")\n        print(f\"  Training Macro F1: {train_f1:.4f}\")\n\n        if run is not None:\n            wandb.log({\n                \"train/epoch_loss\": avg_train_loss,\n                \"train/macro_f1\": train_f1,\n                \"train/accuracy\": train_acc,\n                \"epoch\": epoch + 1\n            })\n            \n        scheduler.step()\n        print(f\"  End of Epoch {epoch+1}. LR Head: {optimizer.param_groups[1]['lr']:.1e}, LR Base: {optimizer.param_groups[0]['lr']:.1e}\")\n            \n        # Run evaluation (ONLY if not using full data)\n        if not USE_FULL_DATA:\n            print(f\"  Running evaluation for Epoch {epoch+1}...\")\n            model.eval()\n            all_preds = []\n            all_labels = []\n            with torch.no_grad():\n                for batch in val_loader:\n                    input_ids = batch['input_ids'].to(device)\n                    attention_mask = batch['attention_mask'].to(device)\n                    labels = batch['labels'].to(device)\n                    outputs = model(input_ids, attention_mask=attention_mask)\n                    sigmoid = torch.sigmoid(outputs.logits)\n                    predictions = (sigmoid > 0.5).int()\n                    all_preds.append(predictions.cpu())\n                    all_labels.append(labels.cpu())\n                    \n            all_preds = torch.cat(all_preds, dim=0).numpy()\n            all_labels = torch.cat(all_labels, dim=0).numpy()\n            macro_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n            print(f\"  Epoch {epoch+1} - Validation Macro F1 (0.5 thresh): {macro_f1:.4f}\")\n        \n        # Save model state based on strategy\n        best_model_state = model.state_dict().copy()\n        \n    print(\"\\n--- Training Finished ---\")\n\n\n    if not USE_FULL_DATA :\n        print(\"\\n--- Calculating Optimal Thresholds on Validation Set ---\")\n        model.eval()\n        val_preds_list = []\n        val_labels_list = []\n        \n        # Get raw probabilities for validation set\n        with torch.no_grad():\n            for batch in tqdm(val_loader, desc=\"Validating\"):\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                labels = batch['labels'].to(device)\n                \n                outputs = model(input_ids, attention_mask=attention_mask)\n                sigmoid = torch.sigmoid(outputs.logits)\n                \n                val_preds_list.append(sigmoid.cpu().numpy())\n                val_labels_list.append(labels.cpu().numpy())\n                \n        val_preds_arr = np.vstack(val_preds_list)\n        val_labels_arr = np.vstack(val_labels_list)\n    \n        # Find best threshold for EACH of the 5 labels\n        optimal_thresholds = []\n        print(\"\\nOptimization Results:\")\n        for i, label in enumerate(emotion_labels):\n            best_f1 = 0\n            best_thresh = 0.5\n            \n            # Sweep thresholds from 0.01 to 0.99\n            for thresh in np.arange(0.01, 1.0, 0.01):\n                pred_binary = (val_preds_arr[:, i] > thresh).astype(int)\n                score = f1_score(val_labels_arr[:, i], pred_binary)\n                \n                if score > best_f1:\n                    best_f1 = score\n                    best_thresh = thresh\n                    \n            optimal_thresholds.append(best_thresh)\n            print(f\"  {label.ljust(10)}: Best Threshold={best_thresh:.3f}, F1-Score={best_f1:.4f}\")\n    \n        print(f\"\\n>>> FINAL OPTIMAL THRESHOLDS: {optimal_thresholds}\")\n    \n    # 4. Save & Upload\n    # Load the best model state\n    if best_model_state is not None:\n        model.load_state_dict(best_model_state)\n\n    save_path = \"./final_model\"\n    print(f\"Saving model locally to {save_path}...\")\n    model.save_pretrained(save_path)\n    tokenizer.save_pretrained(save_path)\n    \n\n    # 5. WandB Finish\n    if run is not None:\n        wandb.finish()\n        print(\"WandB run finished.\")\n\nelse:\n    print(\"SKIPPING TRAINING CELL\")","metadata":{"execution":{"iopub.status.busy":"2025-11-26T15:32:45.544646Z","iopub.execute_input":"2025-11-26T15:32:45.544895Z","iopub.status.idle":"2025-11-26T16:02:07.853790Z","shell.execute_reply.started":"2025-11-26T15:32:45.544870Z","shell.execute_reply":"2025-11-26T16:02:07.852950Z"},"papermill":{"duration":0.024111,"end_time":"2025-11-21T17:12:08.923133","exception":false,"start_time":"2025-11-21T17:12:08.899022","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"W&B login successful.\n\n--- Starting Model Training ---\n","output_type":"stream"},{"name":"stderr","text":"2025-11-26 15:33:07.399479: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764171187.580184      38 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764171187.636991      38 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9636031d5ff40a39d703f660c0fa390"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Using StepLR scheduler\n\n--- Starting Epoch 1/21 ---\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|██████████| 384/384 [01:19<00:00,  4.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Average Training Loss: 0.6562\n  Training Macro F1: 0.5068\n  End of Epoch 1. LR Head: 5.0e-05, LR Base: 5.0e-05\n  Running evaluation for Epoch 1...\n  Epoch 1 - Validation Macro F1 (0.5 thresh): 0.7494\n\n--- Starting Epoch 2/21 ---\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 384/384 [01:19<00:00,  4.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Average Training Loss: 0.3398\n  Training Macro F1: 0.7531\n  End of Epoch 2. LR Head: 2.5e-05, LR Base: 2.5e-05\n  Running evaluation for Epoch 2...\n  Epoch 2 - Validation Macro F1 (0.5 thresh): 0.7709\n\n--- Starting Epoch 3/21 ---\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 384/384 [01:19<00:00,  4.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Average Training Loss: 0.1659\n  Training Macro F1: 0.8862\n  End of Epoch 3. LR Head: 2.5e-05, LR Base: 2.5e-05\n  Running evaluation for Epoch 3...\n  Epoch 3 - Validation Macro F1 (0.5 thresh): 0.8478\n\n--- Starting Epoch 4/21 ---\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 384/384 [01:19<00:00,  4.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Average Training Loss: 0.0831\n  Training Macro F1: 0.9431\n  End of Epoch 4. LR Head: 1.3e-05, LR Base: 1.3e-05\n  Running evaluation for Epoch 4...\n  Epoch 4 - Validation Macro F1 (0.5 thresh): 0.8552\n\n--- Starting Epoch 5/21 ---\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|██████████| 384/384 [01:19<00:00,  4.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Average Training Loss: 0.0470\n  Training Macro F1: 0.9652\n  End of Epoch 5. LR Head: 1.3e-05, LR Base: 1.3e-05\n  Running evaluation for Epoch 5...\n  Epoch 5 - Validation Macro F1 (0.5 thresh): 0.8687\n\n--- Starting Epoch 6/21 ---\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6: 100%|██████████| 384/384 [01:19<00:00,  4.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Average Training Loss: 0.0327\n  Training Macro F1: 0.9780\n  End of Epoch 6. LR Head: 6.3e-06, LR Base: 6.3e-06\n  Running evaluation for Epoch 6...\n  Epoch 6 - Validation Macro F1 (0.5 thresh): 0.8624\n\n--- Starting Epoch 7/21 ---\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7: 100%|██████████| 384/384 [01:19<00:00,  4.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Average Training Loss: 0.0237\n  Training Macro F1: 0.9835\n  End of Epoch 7. LR Head: 6.3e-06, LR Base: 6.3e-06\n  Running evaluation for Epoch 7...\n  Epoch 7 - Validation Macro F1 (0.5 thresh): 0.8670\n\n--- Starting Epoch 8/21 ---\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8: 100%|██████████| 384/384 [01:19<00:00,  4.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Average Training Loss: 0.0187\n  Training Macro F1: 0.9891\n  End of Epoch 8. LR Head: 3.1e-06, LR Base: 3.1e-06\n  Running evaluation for Epoch 8...\n  Epoch 8 - Validation Macro F1 (0.5 thresh): 0.8680\n\n--- Starting Epoch 9/21 ---\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9: 100%|██████████| 384/384 [01:19<00:00,  4.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Average Training Loss: 0.0155\n  Training Macro F1: 0.9899\n  End of Epoch 9. LR Head: 3.1e-06, LR Base: 3.1e-06\n  Running evaluation for Epoch 9...\n  Epoch 9 - Validation Macro F1 (0.5 thresh): 0.8709\n\n--- Starting Epoch 10/21 ---\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10: 100%|██████████| 384/384 [01:19<00:00,  4.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Average Training Loss: 0.0143\n  Training Macro F1: 0.9932\n  End of Epoch 10. LR Head: 1.6e-06, LR Base: 1.6e-06\n  Running evaluation for Epoch 10...\n  Epoch 10 - Validation Macro F1 (0.5 thresh): 0.8674\n\n--- Starting Epoch 11/21 ---\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11: 100%|██████████| 384/384 [01:19<00:00,  4.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Average Training Loss: 0.0127\n  Training Macro F1: 0.9936\n  End of Epoch 11. LR Head: 1.6e-06, LR Base: 1.6e-06\n  Running evaluation for Epoch 11...\n  Epoch 11 - Validation Macro F1 (0.5 thresh): 0.8676\n\n--- Starting Epoch 12/21 ---\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12: 100%|██████████| 384/384 [01:19<00:00,  4.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Average Training Loss: 0.0122\n  Training Macro F1: 0.9942\n  End of Epoch 12. LR Head: 7.8e-07, LR Base: 7.8e-07\n  Running evaluation for Epoch 12...\n  Epoch 12 - Validation Macro F1 (0.5 thresh): 0.8653\n\n--- Starting Epoch 13/21 ---\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13: 100%|██████████| 384/384 [01:19<00:00,  4.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Average Training Loss: 0.0115\n  Training Macro F1: 0.9933\n  End of Epoch 13. LR Head: 7.8e-07, LR Base: 7.8e-07\n  Running evaluation for Epoch 13...\n  Epoch 13 - Validation Macro F1 (0.5 thresh): 0.8704\n\n--- Starting Epoch 14/21 ---\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14: 100%|██████████| 384/384 [01:19<00:00,  4.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Average Training Loss: 0.0111\n  Training Macro F1: 0.9952\n  End of Epoch 14. LR Head: 3.9e-07, LR Base: 3.9e-07\n  Running evaluation for Epoch 14...\n  Epoch 14 - Validation Macro F1 (0.5 thresh): 0.8687\n\n--- Starting Epoch 15/21 ---\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15: 100%|██████████| 384/384 [01:19<00:00,  4.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Average Training Loss: 0.0106\n  Training Macro F1: 0.9944\n  End of Epoch 15. LR Head: 3.9e-07, LR Base: 3.9e-07\n  Running evaluation for Epoch 15...\n  Epoch 15 - Validation Macro F1 (0.5 thresh): 0.8642\n\n--- Starting Epoch 16/21 ---\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16: 100%|██████████| 384/384 [01:19<00:00,  4.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Average Training Loss: 0.0106\n  Training Macro F1: 0.9952\n  End of Epoch 16. LR Head: 2.0e-07, LR Base: 2.0e-07\n  Running evaluation for Epoch 16...\n  Epoch 16 - Validation Macro F1 (0.5 thresh): 0.8670\n\n--- Starting Epoch 17/21 ---\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17: 100%|██████████| 384/384 [01:19<00:00,  4.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Average Training Loss: 0.0102\n  Training Macro F1: 0.9956\n  End of Epoch 17. LR Head: 2.0e-07, LR Base: 2.0e-07\n  Running evaluation for Epoch 17...\n  Epoch 17 - Validation Macro F1 (0.5 thresh): 0.8670\n\n--- Starting Epoch 18/21 ---\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18: 100%|██████████| 384/384 [01:19<00:00,  4.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Average Training Loss: 0.0102\n  Training Macro F1: 0.9956\n  End of Epoch 18. LR Head: 9.8e-08, LR Base: 9.8e-08\n  Running evaluation for Epoch 18...\n  Epoch 18 - Validation Macro F1 (0.5 thresh): 0.8670\n\n--- Starting Epoch 19/21 ---\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19: 100%|██████████| 384/384 [01:19<00:00,  4.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Average Training Loss: 0.0098\n  Training Macro F1: 0.9970\n  End of Epoch 19. LR Head: 9.8e-08, LR Base: 9.8e-08\n  Running evaluation for Epoch 19...\n  Epoch 19 - Validation Macro F1 (0.5 thresh): 0.8664\n\n--- Starting Epoch 20/21 ---\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20: 100%|██████████| 384/384 [01:19<00:00,  4.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Average Training Loss: 0.0100\n  Training Macro F1: 0.9961\n  End of Epoch 20. LR Head: 4.9e-08, LR Base: 4.9e-08\n  Running evaluation for Epoch 20...\n  Epoch 20 - Validation Macro F1 (0.5 thresh): 0.8674\n\n--- Starting Epoch 21/21 ---\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21: 100%|██████████| 384/384 [01:19<00:00,  4.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Average Training Loss: 0.0100\n  Training Macro F1: 0.9961\n  End of Epoch 21. LR Head: 4.9e-08, LR Base: 4.9e-08\n  Running evaluation for Epoch 21...\n  Epoch 21 - Validation Macro F1 (0.5 thresh): 0.8672\n\n--- Training Finished ---\n\n--- Calculating Optimal Thresholds on Validation Set ---\n","output_type":"stream"},{"name":"stderr","text":"Validating: 100%|██████████| 22/22 [00:02<00:00,  9.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nOptimization Results:\n  anger     : Best Threshold=0.630, F1-Score=0.8481\n  fear      : Best Threshold=0.170, F1-Score=0.9177\n  joy       : Best Threshold=0.930, F1-Score=0.8839\n  sadness   : Best Threshold=0.560, F1-Score=0.8679\n  surprise  : Best Threshold=0.850, F1-Score=0.8969\n\n>>> FINAL OPTIMAL THRESHOLDS: [0.63, 0.17, 0.93, 0.56, 0.85]\nSaving model locally to ./final_model...\nWandB run finished.\n","output_type":"stream"}],"execution_count":12},{"id":"9156e979","cell_type":"code","source":"# if DO_TRAIN_AND_UPLOAD :\n#     KAGGLE_USERNAME = 'gaurangnigam'\n#     MODEL_SLUG = 'bert-emotion-classifier'\n#     handle = f'{KAGGLE_USERNAME}/{MODEL_SLUG}/pytorch/v5'\n#     print(f\"Uploading to {handle}...\")\n#     kagglehub.model_upload(handle, save_path, version_notes='Using yangheng/deberta-v3-base-absa-v1.1')\n#     print(\"Model saved. Ready for upload.\")","metadata":{"execution":{"iopub.status.busy":"2025-11-26T16:02:07.854744Z","iopub.execute_input":"2025-11-26T16:02:07.855533Z","iopub.status.idle":"2025-11-26T16:02:07.858892Z","shell.execute_reply.started":"2025-11-26T16:02:07.855513Z","shell.execute_reply":"2025-11-26T16:02:07.858254Z"},"papermill":{"duration":0.011884,"end_time":"2025-11-21T17:12:08.941672","exception":false,"start_time":"2025-11-21T17:12:08.929788","status":"completed"},"scrolled":true,"tags":[],"trusted":true},"outputs":[],"execution_count":13},{"id":"bec16272","cell_type":"markdown","source":"# 4. Inference & Submission","metadata":{"papermill":{"duration":0.007529,"end_time":"2025-11-21T17:12:08.955719","exception":false,"start_time":"2025-11-21T17:12:08.948190","status":"completed"},"tags":[]}},{"id":"fbcf1563","cell_type":"code","source":"if ENSEMBLE_MODELS :\n    print(\"\\n--- Starting Ensemble Inference ---\")\n\n    # The best score came from using a high weight for DeBERTa (0.9) vs BERT (0.1) \n    # MODEL_PATH_0 = \"/kaggle/input/bert-emotion-classifier/pytorch/v4/3\"\n    # MODEL_PATH_1 = \"/kaggle/input/bert-emotion-classifier/pytorch/v4/5\"\n    # MODEL_PATH_2 = \"/kaggle/input/bert-emotion-classifier/pytorch/v3/1\"\n\n    ENSEMBLE_CONFIGS = [\n        {\"name\": \"DeBERTa\", \"path\": MODEL_PATH_0, \"weight\": 0.8, \"model_name_id\": \"microsoft/deberta-v3-base\"},\n        {\"name\": \"DeBERTa\", \"path\": MODEL_PATH_1, \"weight\": 0.1, \"model_name_id\": \"microsoft/deberta-v3-base\"},\n        {\"name\": \"BERT\", \"path\": MODEL_PATH_2, \"weight\": 0.1, \"model_name_id\": \"bert-base-uncased\"},\n    ]\n\n    # Total predictions initialization\n    ensemble_probs = np.zeros((len(test_df), num_labels), dtype=np.float32)\n\n    # --- 2. Sequential Prediction Loop ---\n    for model_info in ENSEMBLE_CONFIGS:\n        model_name = model_info['name']\n        model_path = model_info['path']\n        weight = model_info['weight']\n        \n        print(f\"\\n-> Loading and predicting with {model_name} (Weight: {weight:.2f}) from: {model_path}\")\n        \n        # Load the appropriate model class based on the path/config\n        model_instance = AutoModelForSequenceClassification.from_pretrained(model_path)\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model_instance.to(device)\n        model_instance.eval()\n\n        # Predict\n        model_probs = []\n        with torch.no_grad():\n            for batch in tqdm(test_loader, desc=f\"Predicting ({model_name})\"):\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                outputs = model_instance(input_ids, attention_mask=attention_mask)\n                sigmoid = torch.sigmoid(outputs.logits)\n                model_probs.append(sigmoid.cpu())\n        \n        current_probs = torch.cat(model_probs, dim=0).numpy()\n        \n        # Accumulate Ensemble Probabilities (Weighted Addition)\n        ensemble_probs += current_probs * weight\n\n        # Clean up\n        del model_instance, tokenizer\n        torch.cuda.empty_cache()\n        print(f\"Cleanup complete for {model_name}.\")\n\n    # 3. Apply Manual Thresholds\n    final_preds = np.zeros(ensemble_probs.shape, dtype=int)\n\n    print(\"\\n-> Applying MANUAL thresholds to Ensemble Predictions...\")\n    if MANUAL_THRESHOLDS and len(MANUAL_THRESHOLDS) == num_labels:\n        for i, thresh in enumerate(MANUAL_THRESHOLDS):\n            final_preds[:, i] = (ensemble_probs[:, i] > thresh).astype(int)\n            print(f\"  {emotion_labels[i]}: {thresh}\")\n\n    # 4. Save Submission\n    submission_df = pd.DataFrame(final_preds, columns=emotion_labels)\n    submission_df.insert(0, 'id', test_df['id'])\n    submission_df.to_csv(\"submission.csv\", index=False)\n    print(\"\\nSubmission file created: submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2025-11-26T16:02:07.859655Z","iopub.execute_input":"2025-11-26T16:02:07.859919Z","iopub.status.idle":"2025-11-26T16:02:07.878629Z","shell.execute_reply.started":"2025-11-26T16:02:07.859895Z","shell.execute_reply":"2025-11-26T16:02:07.878023Z"},"papermill":{"duration":0.015511,"end_time":"2025-11-21T17:12:08.977930","exception":false,"start_time":"2025-11-21T17:12:08.962419","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":14},{"id":"e260d780","cell_type":"code","source":"if not ENSEMBLE_MODELS :\n    print(\"\\n--- Starting Inference ---\")\n    \n    # 1. Load Model\n    # bert-emotion-classifier\n    # MODEL_PATH_FOR_INFERENCE = \"/kaggle/input/bert-emotion-classifier/pytorch/v5/5\"\n    MODEL_PATH_FOR_INFERENCE = \"./final_model\" \n    \n    print(f\"Loading model from {MODEL_PATH_FOR_INFERENCE}...\")\n    \n    try:\n        model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH_FOR_INFERENCE)\n        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH_FOR_INFERENCE)\n        model.to(device)\n        model.eval()\n        print(\"Model loaded successfully.\")\n    except Exception as e:\n        print(f\"Error loading model: {e}.\")\n    \n    # 2. Get Thresholds\n    try:\n        final_thresholds = optimal_thresholds \n        print(f\"Using calculated optimal thresholds: {final_thresholds}\")\n    except NameError:\n        print(\"Warning: 'optimal_thresholds' not found. Using MANUAL_THRESHOLDS.\")\n        final_thresholds = MANUAL_THRESHOLDS\n    \n    \n    # 3. Get Test Predictions\n    print(\"Predicting on test set\")\n    test_probs = []\n    with torch.no_grad():\n        for batch in tqdm(test_loader):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            outputs = model(input_ids, attention_mask=attention_mask)\n            sigmoid = torch.sigmoid(outputs.logits)\n            test_probs.append(sigmoid.cpu())\n    \n    test_probs = torch.cat(test_probs, dim=0).numpy()\n    final_preds = np.zeros(test_probs.shape, dtype=int)\n    \n    # 4. Apply Thresholds\n    for i in range(num_labels):\n        # Use the specific threshold for this emotion\n        thresh = final_thresholds[i]\n        final_preds[:, i] = (test_probs[:, i] > thresh).astype(int)\n    \n    # 5. Save Submission\n    submission_df = pd.DataFrame(final_preds, columns=emotion_labels)\n    submission_df.insert(0, 'id', test_df['id'])\n    \n    submission_df.to_csv(\"submission.csv\", index=False)\n    \n    print(\"Submission file created: submission.csv\")\n    \n    submission_df.head()","metadata":{"execution":{"iopub.status.busy":"2025-11-26T16:03:03.490058Z","iopub.execute_input":"2025-11-26T16:03:03.490884Z","iopub.status.idle":"2025-11-26T16:03:09.752468Z","shell.execute_reply.started":"2025-11-26T16:03:03.490860Z","shell.execute_reply":"2025-11-26T16:03:09.751769Z"},"papermill":{"duration":41.528384,"end_time":"2025-11-21T17:12:50.512978","exception":false,"start_time":"2025-11-21T17:12:08.984594","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"\n--- Starting Inference ---\nLoading model from ./final_model...\nModel loaded successfully.\nUsing calculated optimal thresholds: [0.63, 0.17, 0.93, 0.56, 0.85]\nPredicting on test set\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 54/54 [00:05<00:00,  9.03it/s]","output_type":"stream"},{"name":"stdout","text":"Submission file created: submission.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":17},{"id":"4fa86e4a-9559-46aa-95b9-d8f30e06db04","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}